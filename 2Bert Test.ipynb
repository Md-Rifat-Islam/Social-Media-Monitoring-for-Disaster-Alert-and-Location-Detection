{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20281,
     "status": "ok",
     "timestamp": 1701231332939,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "Kx49k-_3SYX6",
    "outputId": "ce0e05ce-80dd-40ff-d420-b36ae51c029a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to access saved files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # Mounts Google Drive to '/content/drive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701231347185,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "onH5JjR51G0C"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained fine-tuned model from pickle file\n",
    "import pickle\n",
    "with open('/content/drive/MyDrive/ML_Project/mlmodel.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1701231349504,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "EwJA04vD5Gn1"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xC98iH04nWn"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and define the device\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "device = torch.device('cuda')  # Use GPU for computation\n",
    "\n",
    "# Test a single sentence for disaster prediction and location extraction\n",
    "test_sentence = \"There is a cyclone in Florida\"\n",
    "\n",
    "# Tokenize the test sentence\n",
    "test_input = tokenizer(\n",
    "    test_sentence, \n",
    "    return_tensors='pt', \n",
    "    truncation=True, \n",
    "    padding=True\n",
    ")\n",
    "test_input = {k: v.to(device) for k, v in test_input.items()}  # Move input to GPU\n",
    "\n",
    "# Perform model inference\n",
    "test_output = model(**test_input)\n",
    "test_prediction = torch.argmax(test_output.logits, dim=-1)  # Get predicted class\n",
    "\n",
    "# Print the disaster prediction result\n",
    "print(f'Test sentence: \"{test_sentence}\" is {\"a disaster\" if test_prediction.item() else \"not a disaster\"}')\n",
    "\n",
    "# Use spaCy for named entity recognition (NER) to extract locations\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')  # Load spaCy English model\n",
    "doc = nlp(test_sentence)  # Process the sentence\n",
    "locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']  # Extract locations (Geo-political entities)\n",
    "print(\"Disaster Locations:\", locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 899,
     "status": "ok",
     "timestamp": 1701231444185,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "VwBbe9ilSyBf"
   },
   "outputs": [],
   "source": [
    "# Process a collection of sentences from a CSV file\n",
    "import pandas as pd\n",
    "path = \"/content/drive/MyDrive/ML_Project/fb_scraped.csv\"  # Path to the CSV file\n",
    "scraped_df = pd.read_csv(path)  # Load the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22811,
     "status": "ok",
     "timestamp": 1701231557023,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "h7IwKkhBTAYf",
    "outputId": "a11f4dcf-e41f-4465-8056-488db00c32ab"
   },
   "outputs": [],
   "source": [
    "# Loop through each text in the 'Text' column of the dataframe\n",
    "for texts in scraped_df['Text']:\n",
    "    test_sentence = texts  # Current sentence\n",
    "\n",
    "    # Tokenize and process the current sentence\n",
    "    test_input = tokenizer(\n",
    "        test_sentence, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        padding=True\n",
    "    )\n",
    "    test_input = {k: v.to(device) for k, v in test_input.items()}  # Move to GPU\n",
    "    test_output = model(**test_input)  # Perform inference\n",
    "    test_prediction = torch.argmax(test_output.logits, dim=-1)  # Get predicted class\n",
    "\n",
    "    # Print the disaster prediction result\n",
    "    print(f'Test sentence: \"{test_sentence}\" is {\"a disaster\" if test_prediction.item() else \"not a disaster\"}')\n",
    "\n",
    "    # Perform NER on the current sentence to extract locations\n",
    "    doc = nlp(test_sentence)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']  # Extract locations\n",
    "    print(\"Disaster Locations:\", locations)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
