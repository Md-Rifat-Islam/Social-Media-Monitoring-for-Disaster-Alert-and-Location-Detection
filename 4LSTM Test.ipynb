{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2930,
     "status": "ok",
     "timestamp": 1701230156333,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "34NQxrD5jV9U"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25869,
     "status": "ok",
     "timestamp": 1701230272460,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "kQg0HO803w8f",
    "outputId": "9d5b2b13-8856-4110-d907-162162b8d5e2"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1612,
     "status": "ok",
     "timestamp": 1701230283042,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "_m2Etk66l9Hv"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained LSTM model from Google Drive\n",
    "with open('/content/drive/MyDrive/ML_Project/lstm.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1701230293028,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "4HErmH_MkZx6"
   },
   "outputs": [],
   "source": [
    "# Load the tokenized tweets dataset\n",
    "path = \"/content/drive/MyDrive/ML_Project/tweets.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df.dropna(subset=['text'])  # Drop rows with missing 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1701230304413,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "mCa_H298oMiu"
   },
   "outputs": [],
   "source": [
    "# Create a tokenizer and fit it on the tweet texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])  # Learn word indices from the dataset\n",
    "total_words = len(tokenizer.word_index) + 1  # Total vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1701230307269,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "p7luOkGPoSu5"
   },
   "outputs": [],
   "source": [
    "# Convert the text data to sequences and pad them\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "padded_sequences = pad_sequences(sequences)  # Pad sequences to uniform length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1701230454536,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "0VE7rwUyOnP4"
   },
   "outputs": [],
   "source": [
    "# Load the scraped dataset for classification\n",
    "path = \"/content/drive/MyDrive/ML_Project/fb_scraped.csv\"\n",
    "scraped_df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UI7UK05WPLis"
   },
   "outputs": [],
   "source": [
    "# Display the scraped data (optional for debugging)\n",
    "print(scraped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1701230711503,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "4Pkk-W_PPfbL",
    "outputId": "7813bd67-3640-4dbe-e5ea-8ff9c11b84d6"
   },
   "outputs": [],
   "source": [
    "# Test the model on a single custom input\n",
    "new_tweet = \"You are Amee Storm, a pink Simic Barbarian who's from the royal court and wields a blood stained tooth\"\n",
    "sequences = tokenizer.texts_to_sequences([new_tweet])  # Tokenize the new tweet\n",
    "padded_sequences = pad_sequences(sequences, maxlen=padded_sequences.shape[1])  # Pad to training input length\n",
    "prediction = model.predict(padded_sequences)\n",
    "\n",
    "# Use a threshold to classify the tweet\n",
    "threshold = 0.5\n",
    "if prediction[0, 0] >= threshold:\n",
    "    print(f'Tweet: \"{new_tweet}\" is a disaster.')\n",
    "else:\n",
    "    print(f'Tweet: \"{new_tweet}\" is not a disaster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1701231213626,
     "user": {
      "displayName": "Md Jamaluddin Tanvin",
      "userId": "06457395428218551143"
     },
     "user_tz": -360
    },
    "id": "AbTms1nJoVe-",
    "outputId": "6528de21-202b-49c2-840e-fc2ef69f1447"
   },
   "outputs": [],
   "source": [
    "# Iterate over the scraped dataset and classify each post\n",
    "for texts in scraped_df['Text']:\n",
    "    new_tweet = texts\n",
    "    sequences = tokenizer.texts_to_sequences([new_tweet])  # Tokenize each text\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=padded_sequences.shape[1])  # Pad to uniform length\n",
    "    prediction = model.predict(padded_sequences)\n",
    "\n",
    "    # Classify based on the threshold\n",
    "    if prediction[0, 0] >= threshold:\n",
    "        print(f'Post: \"{new_tweet[:35]}\" is a disaster.')\n",
    "    else:\n",
    "        print(f'Post: \"{new_tweet[:35]}\" is not a disaster.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
